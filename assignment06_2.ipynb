{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6.2 - More Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
    "Please do **NOT** rename the file!\n",
    "\n",
    "#### State both names of your group members here:\n",
    "[Jane and John Doe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paola Gega, Daniel Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grading Info/Details - Assignment 6.2:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.2.1 - Classification Trees\n",
    "\n",
    "* Implement the Classification Tree Class from scratch using only `NumPy`. The splitting criterion should be Gini impurity. **(RESULT)**\n",
    "* Run your implementation on the `IRIS` classification dataset. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"Base class for a decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=5, min_samples_split=10):\n",
    "        \"\"\"Initialize decision tree.\"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree.\"\"\"\n",
    "        self.tree = self._build_tree(X, y, 0)\n",
    "\n",
    "    def _loss(self, y1, y2):\n",
    "        \"\"\"This method is designed to be overridden by subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_split_candidates(self, feature_values):\n",
    "        \"\"\"\n",
    "        Determines the split candidates for a given feature.\n",
    "        This method is designed to be overridden by subclasses.\n",
    "        By default, it returns unique values, but subclasses like ExtraTree\n",
    "        can implement a different strategy (e.g., random thresholds).\n",
    "        \"\"\"\n",
    "        return np.unique(feature_values)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build the tree.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Make node a leaf if stop conditions are met\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return {'value': y[0]}\n",
    "        elif (depth >= self.max_depth or n_samples < self.min_samples_split):\n",
    "            values, counts = np.unique(y, return_counts=True)\n",
    "            ind = np.argmax(counts)\n",
    "            return {'value': values[ind]}\n",
    "        # Search for optimal split\n",
    "        best_j = None\n",
    "        best_z = None\n",
    "        best_loss = float('inf')\n",
    "        for j in range(n_features):\n",
    "            X_j = self._get_split_candidates(X[:,j])\n",
    "            for z in (X_j[:-1]+X_j[1:])/2:\n",
    "                # Calculate impurity of split\n",
    "                left_indices = X[:, j] <= z\n",
    "                right_indices = ~left_indices\n",
    "                loss = self._loss(y[left_indices], y[right_indices])\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_j = j\n",
    "                    best_z = z\n",
    "        left_indices = X[:, best_j] <= best_z\n",
    "        right_indices = ~left_indices\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return {'feature_index' : best_j, \n",
    "                'threshold' : best_z,\n",
    "                'left' : left_subtree,\n",
    "                'right' : right_subtree}\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for X.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        y_pred = np.empty(n_samples, dtype=float)\n",
    "        for i in range(n_samples):\n",
    "            node = self.tree\n",
    "            while 'value' not in node.keys():\n",
    "                if X[i,node['feature_index']] <= node['threshold']:\n",
    "                    node = node['left']\n",
    "                else:\n",
    "                    node = node['right']\n",
    "            y_pred[i] = node['value']\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "            counts = np.unique(y, return_counts=True)[1]\n",
    "            prob_sq_sum = sum((count / len(y)) ** 2 for count in counts)\n",
    "            return 1 - prob_sq_sum\n",
    "\n",
    "class ClassificationTree(DecisionTree):\n",
    "    \"\"\"Classification decision tree using Gini impurity.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _loss(self, y1, y2):\n",
    "        \"\"\"Calculate Gini impurity of a split.\"\"\"\n",
    "        total_samples = len(y1) + len(y2)\n",
    "        gini1 = gini(y1)\n",
    "        gini2 = gini(y2)\n",
    "        weighted_gini = (len(y1) * gini1 + len(y2) * gini2) / total_samples\n",
    "        return weighted_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.0\n",
      "Accuracy on test set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "# Train a classification tree model where every leaf corresponds to a single sample\n",
    "model = ClassificationTree(max_depth = 150, min_samples_split = 2)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_pred))\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9924812030075187\n",
      "Accuracy on test set: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Train a classification tree model with more reasonable parameters to try to get less overfitting\n",
    "model = ClassificationTree(max_depth = 5, min_samples_split = 20)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_pred))\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.2.2 - Random Forests\n",
    "\n",
    "* Implement Random Forests using only `NumPy`. **(RESULT)**\n",
    "* Compare the results between the random forest run of your `ClassificationTree` class on the `IRIS` dataset. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"Random Forest Classifier.\"\"\"\n",
    "    def __init__(self, bags, bag_size=None, max_depth=5, min_samples_split=10, random_state=None):\n",
    "        self.bags = bags\n",
    "        self.bag_size = bag_size\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the random forest.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.bag_size is None:\n",
    "            self.bag_size = n_samples\n",
    "        n = np.floor(np.sqrt(n_features)).astype(int)\n",
    "        np.random.seed(self.random_state)\n",
    "        for _ in range(self.bags):\n",
    "            sample_indices = np.random.choice(n_samples, self.bag_size, replace=True)\n",
    "            feature_indices = np.random.choice(n_features, n, replace=False)\n",
    "            X_sample, y_sample = X[sample_indices][:,feature_indices], y[sample_indices]\n",
    "\n",
    "            tree = ClassificationTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append((tree, feature_indices))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions by majority voting.\"\"\"\n",
    "        # Collect predictions from each tree\n",
    "        predictions = np.empty((X.shape[0], self.bags), dtype=int)\n",
    "\n",
    "        for i, (tree, feature_indices) in enumerate(self.trees):\n",
    "            # Select the same features as used in fitting\n",
    "            X_sample = X[:, feature_indices]\n",
    "            predictions[:, i] = tree.predict(X_sample)\n",
    "\n",
    "        return np.array([np.bincount(predictions[i]).argmax() for i in range(predictions.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9924812030075187\n",
      "Accuracy on test set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Train a random forest model with the same parameters as before\n",
    "model = RandomForest(bags = 10, max_depth = 5, min_samples_split = 20, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_pred))\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.2.3 - Extra Trees\n",
    "\n",
    "* Implement Extra Trees using only `NumPy`. **(RESULT)**\n",
    "* Compare the results between the `Random Forest` and an `Extra Trees` ensemble implementation on the `IRIS` dataset. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraTree(ClassificationTree):\n",
    "    \"\"\"Extremely Randomized Tree - uses random thresholds instead of optimal ones.\"\"\"\n",
    "    def __init__(self, random_state, *args, **kwargs):\n",
    "        self.random_state = random_state\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _get_split_candidates(self, feature_values):\n",
    "        \"\"\"\n",
    "        Determines the split candidates for a given feature.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        unique_feature_values = np.unique(feature_values)\n",
    "        n_values = len(unique_feature_values)\n",
    "        # num_thresholds = np.random.randint(n_values)+1\n",
    "        num_thresholds = np.floor(np.sqrt(n_values)).astype(int)\n",
    "        indices = np.random.choice(n_values, num_thresholds, replace=False)\n",
    "        return unique_feature_values[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraTrees:\n",
    "    \"\"\"Extremely Randomized Trees ensemble.\"\"\"\n",
    "    def __init__(self, bags, bag_size=None, max_depth=5, min_samples_split=10, random_state=None):\n",
    "        self.bags = bags\n",
    "        self.bag_size = bag_size\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the random forest.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.bag_size is None:\n",
    "            self.bag_size = n_samples\n",
    "        np.random.seed(self.random_state)\n",
    "        for _ in range(self.bags):\n",
    "            sample_indices = np.random.choice(n_samples, self.bag_size, replace=True)\n",
    "            X_sample, y_sample = X[sample_indices], y[sample_indices]\n",
    "\n",
    "            tree = ExtraTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, random_state=self.random_state)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions by averaging all trees.\"\"\"\n",
    "        # Collect predictions from each tree\n",
    "        predictions = np.empty((X.shape[0], self.bags), dtype=int)\n",
    "\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            predictions[:, i] = tree.predict(X)\n",
    "\n",
    "        # Majority voting\n",
    "        return np.array([np.bincount(predictions[i]).argmax() for i in range(predictions.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra tree\n",
      "Accuracy on training set: 0.9849624060150376\n",
      "Accuracy on test set: 0.9333333333333333\n",
      "\n",
      "Extra trees ensemble\n",
      "Accuracy on training set: 0.9097744360902256\n",
      "Accuracy on test set: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Extra Tree\n",
    "extra_tree = ExtraTree(max_depth=5, min_samples_split=5, random_state=42)\n",
    "extra_tree.fit(X_train, y_train)\n",
    "y_train_pred = extra_tree.predict(X_train)\n",
    "y_test_pred = extra_tree.predict(X_test)\n",
    "print(\"Extra tree\")\n",
    "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_test_pred))\n",
    "print()\n",
    "\n",
    "# Create and train the Extra Trees ensemble\n",
    "extra_trees = ExtraTrees(bags=100, max_depth=5, min_samples_split=5, random_state=42)\n",
    "extra_trees.fit(X_train, y_train)\n",
    "y_train_pred = extra_trees.predict(X_train)\n",
    "y_test_pred = extra_trees.predict(X_test)\n",
    "print(\"Extra trees ensemble\")\n",
    "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
