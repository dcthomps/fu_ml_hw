{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2.1 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
    "Please do **NOT** rename the file!\n",
    "\n",
    "#### State both names of your group members here:\n",
    "[Jane and John Doe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Thompson and Paola Gega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Info/Details - Assignment 2.1:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.1 - kMeans\n",
    "\n",
    "kMeans is an unsupervised learning algorithm that partitions n observations into k clusters. Each observation belongs to the cluster with the nearest mean (cluster center or centroid).\n",
    "\n",
    "\n",
    "### 1. kMeans Implementation\n",
    "* Implement the kMeans clustering algorithm using `numpy` only. Use the `KMeans` class structure below. **(RESULT)**\n",
    "* Test the convergence of your implementation by creating a 2D synthetic dataset yourself. Report on the convergence. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, k=3, max_iters=100, tol=1e-4, random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize KMeans clusterer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : int\n",
    "            Number of clusters\n",
    "        max_iters : int\n",
    "            Maximum number of iterations\n",
    "        tol : float\n",
    "            Tolerance for convergence (change in centroids)\n",
    "        random_state : int or None\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Initialize cluster centers using random selection from data points.\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        self.labels_ = np.empty(n, dtype=int)\n",
    "        # Choose k distinct random points from the data to use for the first step of the algorithm\n",
    "        rng = np.random.default_rng(seed=self.random_state)\n",
    "        inds = rng.choice(n, self.k, replace=False)\n",
    "        self.centroids = np.empty((self.k, len(X[0])), dtype=float)\n",
    "        self.centroids = X[inds]\n",
    "    \n",
    "    def initialize_centroids_plusplus(self, X): # for the following Subtask\n",
    "        n = len(X)\n",
    "        self.labels_ = np.empty(n, dtype=int)\n",
    "        rng = np.random.default_rng(seed=self.random_state)\n",
    "        # Initialize first centroid\n",
    "        self.centroids = X[rng.choice(n, 1)]\n",
    "        count = 1\n",
    "        # Draw subsequent centroid points according to knn++ algorithm\n",
    "        while count < self.k:\n",
    "            # Vectorized computation of Euclidean distances.\n",
    "            X_sqnorms = np.sum(X**2, axis=1, keepdims=True)  # (n, 1)\n",
    "            centroids_sqnorms = np.sum(self.centroids**2, axis=1)  # (count, 1) - self.centroids provided as model attribute in this case\n",
    "            dots = np.dot(X, self.centroids.T)  # (n, count)\n",
    "            distances = np.sqrt(X_sqnorms + centroids_sqnorms - 2 * dots)  # X_sqnorms + centroids_sqnorms - broadcasted addition (n, count)\n",
    "            # Construct a vector of probability for drafting the next point\n",
    "            squared_dist = np.min(distances, axis=1)**2\n",
    "            tot = np.sum(squared_dist)\n",
    "            prob = squared_dist / tot\n",
    "            self.centroids = np.vstack([self.centroids, X[rng.choice(n,1,p=prob)]])\n",
    "            count += 1        \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the KMeans model to data X.\n",
    "        \"\"\"\n",
    "        # Nothing to be done here that isn't accounted for by other methods.\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for new data.\n",
    "        \"\"\"\n",
    "        # Vectorized computation of Euclidean distances.\n",
    "        X_new_sqnorms = np.sum(X**2, axis=1, keepdims=True)  # (n_new, 1)\n",
    "        centroids_sqnorms = np.sum(self.centroids**2, axis=1)  # (k, 1) - self.centroids provided as model attribute in this case\n",
    "        dots = np.dot(X, self.centroids.T)  # (n_new, k)\n",
    "        distances = np.sqrt(X_new_sqnorms + centroids_sqnorms - 2 * dots)  # X_new_sqnorms + centroids_sqnorms - broadcasted addition (n_new, k)\n",
    "        # Assign each data point to the cluster corresponding to its closest centroid\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        return labels\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform KMeans clustering and return cluster labels.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        iter = 0\n",
    "        while (iter < self.max_iters):\n",
    "            iter += 1\n",
    "            self.labels_ = self.predict(X)\n",
    "            # Calculate the new centroid of the each class\n",
    "            new_centroids = np.empty((self.k, len(X[0])), dtype=float)\n",
    "            deltas = np.empty(self.k, dtype=float)\n",
    "            for i in range(self.k):\n",
    "                new_centroids[i] = np.mean(X[self.labels_ == i], axis = 0)\n",
    "                deltas[i] = np.sqrt(np.sum((self.centroids[i] - new_centroids[i])**2))\n",
    "            # Algorithm terminates if convergence condition is met\n",
    "            if np.max(deltas[i])<self.tol:\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "        if iter == self.max_iters:\n",
    "            print(\"Did not converge within\", self.max_iters, \"iterations!\")\n",
    "        else:\n",
    "            print(\"Converged after\", iter, \"iterations!\")\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 24 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Generate some synthetic 2-dimensional data\n",
    "X = np.vstack((np.random.multivariate_normal((1,0), [[1, 0], [0, 4]], 1000),np.random.multivariate_normal((0,1), [[4, 0], [0, 1]], 1000),np.random.multivariate_normal((0,0), [[3, 1], [1, 3]], 1000)))\n",
    "true_labels = np.zeros(3000, dtype=int)\n",
    "true_labels[1000:2000] = 1\n",
    "true_labels[2000:3000] = 2\n",
    "\n",
    "# Run kMeans on the data\n",
    "KM = KMeans(k=3)\n",
    "KM.initialize_centroids(X)\n",
    "predicted_labels = KM.fit_predict(X)\n",
    "# Let's see how many points were grouped into each cluster.\n",
    "# for i in range(3):\n",
    "#     print(np.sum(predicted_labels == i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We are sometimes getting a message that we are taking the square root of a negative number.  This part of the code was taken directly from Manuel's sample code.  We are assuming that it is just due to a rounding error, and it does not seem to prevent the algorithm from working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. kMeans++ initialization\n",
    "\n",
    "* Implement the kMeans++ initialization method in the KMeans class. **(RESULT)**\n",
    "* Compare the convergence speed of kMeans with random initialization and kMeans++ initialization on your synthetic dataset from Part 1. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 16 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Run kMeans on the data after using the kMeans++ initialization function\n",
    "KM = KMeans(k=3)\n",
    "KM.initialize_centroids_plusplus(X)\n",
    "predicted_labels = KM.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report:** kMeans++ converged in one third fewer steps in this case!  Usually it appears to be a smaller fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualization of Cluster Quality\n",
    "\n",
    "\n",
    "* Visualize the clustering results of your kMeans implementation on a synthetic 2D dataset with at least 4 clusters using matplotlib. **(RESULT)**\n",
    "* Determine the optimal number of clusters using the elbow method. Report on your findings using a simple plot. **(RESULT)**\n",
    "* Report on the silhouette score of your clustering results for the optimal k and k-1. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.2 - DBSCAN (BONUS)\n",
    "\n",
    "DBSCAN is a density-based clustering algorithm that groups together points that are closely packed together, marking outliers points that lie alone in low-density regions.\n",
    "\n",
    "* Implement the DBSCAN algorithm using `numpy` only. Use the `DBSCAN` class structure below. **(RESULT)**\n",
    "* Test your DBSCAN implementation on a synthetic 2D dataset with noise. Visualize the clustering results using matplotlib. **(RESULT)**\n",
    "* Compare the performance of your DBSCAN implementation with your kMeans implementation on the same synthetic 2D dataset using silhouette score as a metric. Please use the same random seed to make it comparable. **(RESULT)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque   # Useful for efficient BFS implementation (FIFO) for iterating through neighboring points\n",
    "\n",
    "class DBSCAN:\n",
    "    def __init__(self, eps=0.5, min_samples=5, metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        eps : float\n",
    "            Maximum distance between two samples for them to be considered neighbors\n",
    "        min_samples : int\n",
    "            Number of samples in a neighborhood for a point to be considered a core point\n",
    "            (including the point itself)\n",
    "        metric : str\n",
    "            Distance metric to use ('euclidean' or 'manhattan')\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.metric = metric\n",
    "        self.labels_ = None\n",
    "        self.core_sample_indices_ = None\n",
    "        self.components_ = None\n",
    "        self.n_clusters_ = None\n",
    "        self.n_noise_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X_new):\n",
    "        \"\"\"\n",
    "        Predict the closest cluster for new points.\n",
    "        Note: New points can only be assigned to existing clusters or marked as noise.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this function\n",
    "        pass\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering and return cluster labels.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
