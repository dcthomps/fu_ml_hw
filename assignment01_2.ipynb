{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.2 - KNNR / DNNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Assignment Guidelines\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Weekly Assignment Structure\n",
    "\n",
    "Each week includes **two assignments**:\n",
    "\n",
    "| File Format | Required For |\n",
    "|-------------|--------------|\n",
    "| `assignment[x]_1.ipynb` | All students |\n",
    "| `assignment[x]_2.ipynb` | 10 ECTS students only |\n",
    "\n",
    "Upload your solution as a `.ipynb` file **AND** a `.pdf` file in the respective entry in the whiteboard. You will find each deadline there as well.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Grading Requirements\n",
    "\n",
    "### Pass/Fail System\n",
    "* Each submitted `.ipynb` notebook receives either a **\"pass\"** or **\"fail\"** grade\n",
    "* **Minimum # of passes: n-1** \n",
    "\n",
    "### Separate Tracking\n",
    "The n-1 # of passes is tracked **separately** for each assignment type:\n",
    "\n",
    "#### For 5 ECTS Students:\n",
    "- ‚úîÔ∏è Must achieve n-1 # of passes on `assignment[x]_1.ipynb` series\n",
    "\n",
    "#### For 10 ECTS Students:\n",
    "- ‚úîÔ∏è Must achieve n-1 # of passes on `assignment[x]_1.ipynb` series\n",
    "- ‚úîÔ∏è Must achieve n-1 # of passes on `assignment[x]_2.ipynb` series\n",
    "\n",
    "---\n",
    "\n",
    "## üåü BONUS Tasks\n",
    "\n",
    "- **Optional BONUS tasks** appear in some notebooks\n",
    "- Successfully completing **BONUS** tasks earns **extra points** toward your final grade\n",
    "- Check the **Whiteboard** assignment grading for bonus point confirmations\n",
    "- 10 ECTS students will find separate bonus tasks in `assignment[x]_2.ipynb`\n",
    "- 5 ECTS students will **NOT** get additional extra points for solving `assignment[x]_2.ipynb` Bonus tasks\n",
    "- Vice Versa: 10 ECTS students will **NOT** get additional extra points for solving `assignment[x]_1.ipynb` Bonus tasks\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Quick Summary\n",
    "\n",
    "| Study Program | Requirements | Pass Criteria |\n",
    "|---------------|-------------|---------------|\n",
    "| **5 ECTS** | `assignment[x]_1.ipynb` | n-1 |\n",
    "| **10 ECTS** | `assignment[x]_1.ipynb` + `assignment[x]_2.ipynb` | n-1 each |\n",
    "| **All Students** | Optional BONUS tasks | Extra final exam points |\n",
    "\n",
    "\n",
    "**Note:** The n-1 rule is mandatory to pass the tutorial component of the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
    "Please do **NOT** rename the file!\n",
    "\n",
    "#### State both names of your group members here:\n",
    "[Jane and John Doe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Thompson and Paola Gega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Info/Details - Assignment 1.2:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
    "\n",
    "Note: For now, we will keep the test cases black-boxed, meaning we won't reveal their internal implementation details. This might be subject to change in the future, depending on how well we can keep up writing test cases for upcoming assignments which are still work in progress. (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2.1 - kNN on House Pricing Data (KNNR)\n",
    "\n",
    "Implement the k-Nearest Neighbor (kNN) algorithm from scratch using only NumPy. <br><br> You may use the provided sklearn functions to load datasets and evaluate your results. Apply your kNN implementation on a regression Problem - the [California Housing Dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices).\n",
    "\n",
    "* Show 5 data samples to get an idea of the data. **(RESULT)**\n",
    "* Split the dataset into a training set (80%) and a test set (20%). Train your model on the training set and evaluate it on the test set using the mean squared error (MSE) as a metric. Try at least 3 different values for k (e.g., 1, 3, 5) and 2 different distance measures (e.g., Euclidean, Manhattan). Report on your results. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get California Housing Data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "# print(X.shape, y.shape)\n",
    "\n",
    "# Split data and normalize features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "X_train = (X_train-X_train.mean(axis=0))/X_train.std(axis=0)\n",
    "X_test = (X_test-X_test.mean(axis=0))/X_test.std(axis=0)\n",
    "\n",
    "# Choose five random samples from the training set\n",
    "rand_inds = np.array(np.random.choice(len(y_train), 5, replace=False)) # generate 5 random indices\n",
    "# First project on the pricipal components\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "values, vectors = np.linalg.eig(cov_matrix)\n",
    "sorted_indices = np.argsort(values)[::-1]\n",
    "values_sorted = values[sorted_indices]\n",
    "vectors_sorted = vectors[:,sorted_indices]\n",
    "pca_components = vectors_sorted[:, :2]\n",
    "projected_data = np.dot(X_train[rand_inds], pca_components)\n",
    "# Plot the projection\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1])\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.show()\n",
    "\n",
    "# Make sure epsilon in suitably small for our data\n",
    "ep = np.min(X.std(axis=0)) * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRegressor:\n",
    "    \"\"\"Your KNN Regressor implementation\"\"\"\n",
    "    def __init__(self, k=3, p=2):\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Fit the model using the training data.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Predict label for multiple samples.\"\"\"\n",
    "        n_test = len(X_test)\n",
    "        y_test_hat = np.empty(n_test, dtype=float) # initialise predicted label array\n",
    "        \n",
    "        for i in np.arange(n_test): # go through test samples\n",
    "            dist = np.linalg.norm(self.X_train - X_test[i], ord=self.p, axis=1) # compute L_p distance of every training sample to test sample at hand\n",
    "            sorted_ind = np.argsort(dist)[:self.k] # indices of k closest neighbors\n",
    "            \"\"\"Take a distance-weighted average of nearest_lables\"\"\"\n",
    "            y_test_hat[i]= np.sum(self.y_train[sorted_ind] / (dist[sorted_ind]+ep))/np.sum(1/(dist[sorted_ind]+ep))\n",
    "\n",
    "        return y_test_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean metric:\n",
      "Mean squared error for k=1: 0.62089093821141\n",
      "Mean squared error for k=3: 0.4514473266673486\n",
      "Mean squared error for k=5: 0.4174448580646762\n",
      "Mean squared error for k=7: 0.40776637242886143\n",
      "Mean squared error for k=9: 0.4072961048610772\n",
      "Manhattan metric:\n",
      "Mean squared error for k=1: 0.5612838357935077\n",
      "Mean squared error for k=3: 0.4110802957372379\n",
      "Mean squared error for k=5: 0.37459870631059533\n",
      "Mean squared error for k=7: 0.36200170861947145\n",
      "Mean squared error for k=9: 0.363318386756066\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean metric:\")\n",
    "for k in [1,3,5,7,9]:\n",
    "    KNN = KNNRegressor(k)\n",
    "    KNN.fit(X_train, y_train)\n",
    "    y_test_hat = KNN.predict(X_test)\n",
    "    print(\"Mean squared error for k={}:\".format(k), np.mean((y_test_hat - y_test)**2))\n",
    "print(\"Manhattan metric:\")\n",
    "for k in [1,3,5,7,9]:\n",
    "    KNN = KNNRegressor(k,p=1)\n",
    "    KNN.fit(X_train, y_train)\n",
    "    y_test_hat = KNN.predict(X_test)\n",
    "    print(\"Mean squared error for k={}:\".format(k), np.mean((y_test_hat - y_test)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "We tested a few larger values for k and the mean squared error does not seems to drop off much after this.  Therefore k=5 is probably a reasonable choice of the hyperparameter in this application.  The Manhattan metric may yield a marginal improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2.2 - DNNR\n",
    "\n",
    "Implement the DNNR algorithm yourself and apply it to the same regression problem from Task 1.2.1. <br><br> You may use the provided sklearn functions to load datasets and evaluate your results. Apply your kNN implementation on a regression Problem - the [California Housing Dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices).\n",
    "\n",
    "* Split the dataset into a training set (80%) and a test set (20%). Train your model on the training set and evaluate it on the test set using the mean squared error (MSE) as a metric. Try at least 3 different values for k (e.g., 1, 3, 5) and report the results. **(RESULT)**\n",
    "* How does the predicition quality of DNNR compare to kNN? Discuss your results. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNRegressor:\n",
    "    \"\"\"DNNR - uses local gradients for Taylor approximation\"\"\"\n",
    "    def __init__(self, k=5, p=2, k_gradient=10):\n",
    "        self.k = k  # neighbors for prediction\n",
    "        self.p = p\n",
    "        self.k_gradient = k_gradient  # neighbors for gradient estimation\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Fit the model using the training data.\"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        n_train = len(X_train)\n",
    "        self.gamma_hat = np.empty((n_train,len(X_train[0])), dtype=float) # initialise array of gradients\n",
    "        for i in np.arange(n_train): # go through test samples\n",
    "            dist = np.linalg.norm(self.X_train - self.X_train[i], ord=self.p, axis=1) # compute L_p distance of every training sample to test sample at hand\n",
    "            sorted_ind = np.argsort(dist)[1:self.k_gradient+1] # indices of k_gradient closest neighbors\n",
    "            \"\"\"Estimate the gradient near sample based on the labels of neighboring points\"\"\"\n",
    "            regressors = self.X_train[sorted_ind] - self.X_train[i]\n",
    "            self.gamma_hat[i] = np.linalg.lstsq(regressors, self.y_train[sorted_ind]-self.y_train[i])[0]\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Predict label for multiple samples.\"\"\"\n",
    "        n_test = len(X_test)\n",
    "        y_test_hat = np.empty(n_test, dtype=float) # initialise predicted label array\n",
    "        \n",
    "        for i in np.arange(n_test): # go through test samples\n",
    "            dist = np.linalg.norm(self.X_train - X_test[i], ord=self.p, axis=1) # compute L_p distance of every training sample to test sample at hand\n",
    "            sorted_ind = np.argsort(dist)[:self.k] # indices of k closest neighbors\n",
    "            \"\"\"Take the average of the labels predicted for x\n",
    "             based on first-order approximations of the model near neighboring points\"\"\"\n",
    "            y_test_hat[i]= np.mean(self.y_train[sorted_ind]) + np.trace(np.dot(X_test[i]-self.X_train[sorted_ind],self.gamma_hat[sorted_ind].T))/self.k\n",
    "\n",
    "        return y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean metric:\n",
      "Mean squared error for k=1: 0.5356724258037279\n",
      "Mean squared error for k=3: 0.42472885602881005\n",
      "Mean squared error for k=5: 0.39110896731275097\n",
      "Mean squared error for k=7: 0.37169161307163423\n",
      "Manhattan metric:\n",
      "Mean squared error for k=1: 0.5911207422428205\n",
      "Mean squared error for k=3: 0.4153274676591327\n",
      "Mean squared error for k=5: 0.6012734290523478\n",
      "Mean squared error for k=7: 1.469021820776425\n"
     ]
    }
   ],
   "source": [
    "print(\"Euclidean metric:\")\n",
    "for k in [1,3,5,7]:\n",
    "    DNN = DNNRegressor(k, k_gradient=30)\n",
    "    DNN.fit(X_train, y_train)\n",
    "    y_test_hat = DNN.predict(X_test)\n",
    "    print(\"Mean squared error for k={}:\".format(k), np.mean((y_test_hat - y_test)**2))\n",
    "\n",
    "print(\"Manhattan metric:\")\n",
    "for k in [1,3,5,7]:\n",
    "    DNN = DNNRegressor(k, p=1, k_gradient=30)\n",
    "    DNN.fit(X_train, y_train)\n",
    "    y_test_hat = DNN.predict(X_test)\n",
    "    print(\"Mean squared error for k={}:\".format(k), np.mean((y_test_hat - y_test)**2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "\n",
    "In general the differential version seemed to perform similarly to slightly worse at prediction than the distance-weighted normal KNNR.  We tested a few choices of k_gradient and 30 to 40 seemed to do well, but this could be optimized.\n",
    "\n",
    "Note that for a few training splits the differential KNNR actually performs quite poorly.  This may indicate that housing price is not far from being a \"locally constant\" function of the given features.  Alternatively, far-away points could be contributing too much to our gradient estimation.  Taking a lower value for k_gradient may improve the model's resiliency.\n",
    "\n",
    "Some kind of distance-weighting could be done both when estimating gradients and labels to improve the quality of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
