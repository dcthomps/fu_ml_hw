{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6.1 - Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
    "Please do **NOT** rename the file!\n",
    "\n",
    "#### State both names of your group members here:\n",
    "[Jane and John Doe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Thompson and Paola Gega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grading Info/Details - Assignment 6.1:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.1.1 - Regression Trees\n",
    "\n",
    "* Implement the Regression Tree Class from scratch using only `NumPy`. **(RESULT)**\n",
    "* Run your implementation on the synthetic regression dataset provided. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(n_samples=1000, n_features=8, noise=0.1, random_state=42):\n",
    "    \"\"\"Generate synthetic regression data similar to California housing.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create target with non-linear relationships\n",
    "    y = (2.5 * X[:, 0] +                     # Linear relationship\n",
    "          1.8 * X[:, 1] ** 2 +               # Quadratic (non-linear)\n",
    "          -1.2 * X[:, 2] * X[:, 3] +         # Interaction between features\n",
    "          0.5 * np.sin(5 * X[:, 4]) +        # Sinusoidal (periodic pattern)\n",
    "          0.8 * X[:, 5] +                    # Linear\n",
    "          -0.3 * X[:, 6] ** 3 +              # Cubic (strong non-linearity)\n",
    "          1.5 * X[:, 7])                     # Linear\n",
    "    \n",
    "    # Add noise\n",
    "    y += noise * np.random.randn(n_samples)\n",
    "    \n",
    "    # Scale to reasonable range\n",
    "    y = (y - y.min()) / (y.max() - y.min()) * 4 + 1\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  # Index of the feature to split on\n",
    "        self.threshold = threshold          # Value of the feature to split at\n",
    "        self.left = left                    # Left subtree\n",
    "        self.right = right                  # Right subtree\n",
    "        self.value = value                  # Value if it's a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "    \"\"\"A binary decision tree for regression using numpy.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=5, min_samples_split=10):\n",
    "        \"\"\"\n",
    "        Initialize Regression tree.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_depth : int\n",
    "            Maximum depth.\n",
    "        min_samples_split : int\n",
    "            Number of samples beneath which we do not continue refining \n",
    "            the decision tree. \n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the regression tree.\"\"\"\n",
    "        self.root = self._build_tree(X, y, 0)\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"Recursively build the tree.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Make node a leaf if stop conditions are met\n",
    "        if (depth >= self.max_depth) or (n_samples < self.min_samples_split):\n",
    "            return Node(value=np.mean(y))\n",
    "        # Search for optimal split\n",
    "        best_j = None\n",
    "        best_z = None\n",
    "        best_loss = float('inf')\n",
    "        for j in range(n_features):\n",
    "            X_j = np.unique(X[:,j])\n",
    "            for z in (X_j[:-1]+X_j[1:])/2:\n",
    "                # Calculate impurity of split\n",
    "                left_indices = X[:, j] <= z\n",
    "                right_indices = ~left_indices\n",
    "                val_l = np.mean(y[left_indices])\n",
    "                val_r = np.mean(y[right_indices])\n",
    "                loss = (np.sum((y[left_indices] - val_l)**2) \n",
    "                        + np.sum((y[right_indices] - val_r)**2)) / n_samples\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_j = j\n",
    "                    best_z = z\n",
    "        # Enter data for a non-leaf node\n",
    "        left_indices = X[:, best_j] <= best_z\n",
    "        right_indices = ~left_indices\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return Node(feature_index=best_j, threshold=best_z,\n",
    "                    left=left_subtree, right=right_subtree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for X.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        y_pred = np.empty(n_samples, dtype=float)\n",
    "        for i in range(n_samples):\n",
    "            node = self.root\n",
    "            while node.value is None:\n",
    "                if X[i,node.feature_index] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            y_pred[i] = node.value\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set: 0.0\n",
      "MSE on test set: 0.06723361122808473\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_regression_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Train a regression tree model where every leaf corresponds to a single sample\n",
    "model = RegressionTree(max_depth= 1000, min_samples_split=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"MSE on training set:\", mean_squared_error(y_train, y_train_pred))\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"MSE on test set:\", mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set: 0.027999299979648406\n",
      "MSE on test set: 0.05981782480330567\n"
     ]
    }
   ],
   "source": [
    "# Train a regression tree model with more reasonable parameters to try to get less overfitting\n",
    "model = RegressionTree(max_depth= 10, min_samples_split=20)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"MSE on training set:\", mean_squared_error(y_train, y_train_pred))\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"MSE on test set:\", mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.1.2 - Bagging\n",
    "\n",
    "* Implement Bagging using only `NumPy`. **(RESULT)**\n",
    "* Compare the results between the bagged run of your `RegressionTree` class on the synthetic dataset. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingRegressor:\n",
    "    \"\"\"Bagging ensemble for regression trees.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        # TODO: Implement this function\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Fit the bagging ensemble.\"\"\"\n",
    "        pass\n",
    "        # TODO: Implement this function\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Make predictions by averaging all trees.\"\"\"\n",
    "        pass\n",
    "        # TODO: Implement this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
