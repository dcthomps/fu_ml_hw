{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6.1 - Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
    "Please do **NOT** rename the file!\n",
    "\n",
    "#### State both names of your group members here:\n",
    "[Jane and John Doe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Thompson and Paola Gega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grading Info/Details - Assignment 6.1:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.1.1 - Regression Trees\n",
    "\n",
    "* Implement the Regression Tree Class from scratch using only `NumPy`. **(RESULT)**\n",
    "* Run your implementation on the synthetic regression dataset provided. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(n_samples=1000, n_features=8, noise=0.1, random_state=42):\n",
    "    \"\"\"Generate synthetic regression data similar to California housing.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Create target with non-linear relationships\n",
    "    y = (2.5 * X[:, 0] +                     # Linear relationship\n",
    "          1.8 * X[:, 1] ** 2 +               # Quadratic (non-linear)\n",
    "          -1.2 * X[:, 2] * X[:, 3] +         # Interaction between features\n",
    "          0.5 * np.sin(5 * X[:, 4]) +        # Sinusoidal (periodic pattern)\n",
    "          0.8 * X[:, 5] +                    # Linear\n",
    "          -0.3 * X[:, 6] ** 3 +              # Cubic (strong non-linearity)\n",
    "          1.5 * X[:, 7])                     # Linear\n",
    "    \n",
    "    # Add noise\n",
    "    y += noise * np.random.randn(n_samples)\n",
    "    \n",
    "    # Scale to reasonable range\n",
    "    y = (y - y.min()) / (y.max() - y.min()) * 4 + 1\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "    \"\"\"A binary decision tree for regression using numpy.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=5, min_samples_split=10):\n",
    "        \"\"\"\n",
    "        Initialize Regression tree.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_depth : int\n",
    "            Maximum depth. If max_depth = -1 then there is no limit.\n",
    "        min_samples_split : int\n",
    "            Number of samples beneath which we do not continue refining \n",
    "            the decision tree. \n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_samples = None\n",
    "        self.n_features = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.feature = None\n",
    "        self.spl_point = None\n",
    "        self.pred_val = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the regression tree.\"\"\"\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self._build_tree(X, y)\n",
    "    \n",
    "    def _build_tree(self, X, y):\n",
    "        \"\"\"Recursively build the tree.\"\"\"\n",
    "        # Make node a leaf if stop conditions are met\n",
    "        if (self.max_depth == 0) or (self.n_samples < self.min_samples_split):\n",
    "            self.pred_val = np.mean(y)\n",
    "            return\n",
    "        # Search for optimal split\n",
    "        best_j = None\n",
    "        best_z = None\n",
    "        best_loss = float('inf')\n",
    "        for j in range(self.n_features):\n",
    "            X_j = np.sort(X[:,j])\n",
    "            for z in (X_j[:-1] + X_j[1:])/2:\n",
    "                # Calculate impurity of split\n",
    "                val_l = np.mean(y[X_j <= z])\n",
    "                val_r = np.mean(y[X_j > z])\n",
    "                loss = np.mean((y[X_j <= z] - val_l)**2) + np.mean((y[X_j > z] - val_r)**2)\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_j = j\n",
    "                    best_z = z\n",
    "        # Enter data for a non-leaf node\n",
    "        self.feature = best_j\n",
    "        self.spl_point = best_z\n",
    "        self.left = RegressionTree(max_depth = self.max_depth - 1,\n",
    "                                   min_samples_split = self.min_samples_split)\n",
    "        self.left.fit(X[X_j <= z], y[X_j <= z])\n",
    "        self.right = RegressionTree(max_depth = self.max_depth - 1,\n",
    "                                   min_samples_split = self.min_samples_split)\n",
    "        self.right.fit(X[X_j > z], y[X_j > z])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for X.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        y_pred = np.empty(n_samples, dtype=float)\n",
    "        for i in range(n_samples):\n",
    "            node = self\n",
    "            while (not node.pred_val):\n",
    "                if X[i,self.feature] <= self.spl_point:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            y_pred[i] = node.pred_val\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set: 0.28799318751344305\n",
      "MSE on test set: 0.20757899158237966\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_regression_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Train a regression tree model where every leaf corresponds to a single sample\n",
    "model = RegressionTree(max_depth= - 1, min_samples_split=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"MSE on training set:\", mean_squared_error(y_train, y_train_pred))\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"MSE on test set:\", mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.1.2 - Bagging\n",
    "\n",
    "* Implement Bagging using only `NumPy`. **(RESULT)**\n",
    "* Compare the results between the bagged run of your `RegressionTree` class on the synthetic dataset. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingRegressor:\n",
    "    \"\"\"Bagging ensemble for regression trees.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        # TODO: Implement this function\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Fit the bagging ensemble.\"\"\"\n",
    "        pass\n",
    "        # TODO: Implement this function\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Make predictions by averaging all trees.\"\"\"\n",
    "        pass\n",
    "        # TODO: Implement this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
