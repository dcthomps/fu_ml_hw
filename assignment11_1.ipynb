{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11.1 - Transformer\n",
    "\n",
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please state both names of your group members here:\n",
    "Jane and John Doe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paola Gega, Daniel Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11.1.1: Self-Attention\n",
    "\n",
    "Implement the attention mechanism by yourself. You are free to use torch and numpy to speed up the matrix multiplications, but please don't just use their transformer implementation.\n",
    "\n",
    "In the image below, you see the design of one Encoder Block. We want you to set up this Block. Please use your implementation of the Self-Attention (doesn't have to be multi-head) and build the Add & Norm and Feed Forward layers on top of it. Add & Norm and the Feed Forward should be implementations by PyTorch or else. You only need to use your own Self-Attention function.\n",
    "\n",
    "* Show that your model block works, by forwarding a randomly initialized tensor through it once. Print the values of the Random input tensor, the output tensor and the Q,K and V matrices. **(RESULT)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\", height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionEncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_head, dim_feedforward, dropout=0.1):\n",
    "        super(SelfAttentionEncoderBlock, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Initialize weights for attention and feedforward layers\n",
    "        self.W_q = torch.nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_k = torch.nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_v = torch.nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_o = torch.nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(d_model, dim_feedforward))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(dim_feedforward, d_model))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n",
    "        return e_x / e_x.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention\n",
    "        Q = torch.matmul(src, self.W_q)\n",
    "        K = torch.matmul(src, self.W_k)\n",
    "        V = torch.matmul(src, self.W_v)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
    "        attn_weights = self.softmax(scores)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        attn_output = torch.matmul(attn_output, self.W_o)\n",
    "\n",
    "        src = src + attn_output  # Add & Norm\n",
    "        # Feedforward\n",
    "        ff_output = torch.relu(torch.matmul(src, self.W1))  # ReLU activation\n",
    "        ff_output = torch.matmul(ff_output, self.W2)\n",
    "\n",
    "        src = src + ff_output  # Add & Norm\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[[-0.1013, -0.4890],\n",
      "         [-1.3201,  0.4021]],\n",
      "\n",
      "        [[-0.3013,  0.5486],\n",
      "         [ 0.3052, -0.4288]]])\n",
      "Output Tensor:\n",
      " tensor([[[-0.6798, -1.4005],\n",
      "         [-5.2709, -7.2139]],\n",
      "\n",
      "        [[-1.3682, -0.6569],\n",
      "         [ 0.6920, -0.0146]]], grad_fn=<AddBackward0>)\n",
      "Q Matrix:\n",
      " tensor([[[-0.0914, -0.7301],\n",
      "         [-0.6554, -0.7728]],\n",
      "\n",
      "        [[-0.1135,  0.4129],\n",
      "         [ 0.1250, -0.2547]]], grad_fn=<UnsafeViewBackward0>)\n",
      "K Matrix:\n",
      " tensor([[[-0.8854,  0.1660],\n",
      "         [ 1.6493,  0.9159]],\n",
      "\n",
      "        [[ 1.2656,  0.1250],\n",
      "         [-1.0351, -0.1499]]], grad_fn=<UnsafeViewBackward0>)\n",
      "V Matrix:\n",
      " tensor([[[-0.2667,  0.4326],\n",
      "         [ 0.0842,  0.1197]],\n",
      "\n",
      "        [[ 0.2592, -0.3447],\n",
      "         [-0.1959,  0.2459]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_model = 2\n",
    "n_head = 8\n",
    "dim_feedforward = 16\n",
    "seq_length = 2\n",
    "batch_size = 2\n",
    "encoder_block = SelfAttentionEncoderBlock(d_model, n_head, dim_feedforward)\n",
    "src = torch.randn(seq_length, batch_size, d_model)\n",
    "output = encoder_block.forward(src)\n",
    "\n",
    "print(\"Input Tensor:\\n\", src)\n",
    "print(\"Output Tensor:\\n\", output)\n",
    "Q = torch.matmul(src, encoder_block.W_q)\n",
    "K = torch.matmul(src, encoder_block.W_k)\n",
    "V = torch.matmul(src, encoder_block.W_v)\n",
    "print(\"Q Matrix:\\n\", Q)\n",
    "print(\"K Matrix:\\n\", K)\n",
    "print(\"V Matrix:\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11.2 Use your own Transformer Block\n",
    "\n",
    "* Chain 3 of your transformer blocks to set up a model. Put 1 fully connected layer head on top. **(RESULT)**\n",
    "* Train your model on the MNIST dataset for image classification. **(RESULT)**\n",
    "* Report the test accuracy after training. **(RESULT)**\n",
    "\n",
    "Can you make your own attention work? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# We couldn't get any decent training results with our implementation so we have to\n",
    "# use the built-in attention unfortunately.\n",
    "# Maybe we just weren't training long enough but accuracy did not seem better than a random guess.\n",
    "\n",
    "class SelfAttentionEncoderBlock(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_head, dim_feedforward, dropout=0.1):\n",
    "        super(SelfAttentionEncoderBlock, self).__init__()\n",
    "        self.self_attn = torch.nn.MultiheadAttention(d_model, n_head, dropout=dropout)\n",
    "        self.linear1 = torch.nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear2 = torch.nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention\n",
    "        src2, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Feedforward\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_head, dim_feedforward, num_layers, num_classes, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            SelfAttentionEncoderBlock(d_model, n_head, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = torch.nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "        # Take the mean across the sequence length dimension\n",
    "        src = src.mean(dim=0)\n",
    "        output = self.fc(src)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2508094012737274\n",
      "Epoch 2, Loss: 0.197285994887352\n",
      "Epoch 3, Loss: 0.11669344455003738\n",
      "Epoch 4, Loss: 0.27959802746772766\n",
      "Epoch 5, Loss: 0.21897737681865692\n",
      "Test Accuracy: 95.07%\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "n_head = 8\n",
    "dim_feedforward = 2048\n",
    "num_layers = 3\n",
    "num_classes = 10\n",
    "\n",
    "# Train\n",
    "n_epochs = 5\n",
    "model = TransformerModel(d_model, n_head, dim_feedforward, num_layers, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "for epoch in range(n_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten images and project to d_model dimensions\n",
    "        images = images.view(images.size(0), -1)  # Flatten\n",
    "        images = torch.nn.functional.pad(images, (0, d_model - images.size(1)), \"constant\", 0)  # Pad to d_model\n",
    "        images = images.unsqueeze(0)  # Add sequence length dimension\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), -1)\n",
    "        images = torch.nn.functional.pad(images, (0, d_model - images.size(1)), \"constant\", 0)\n",
    "        images = images.unsqueeze(0)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report:** Trained only for five epochs, but kind of lackluster results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
