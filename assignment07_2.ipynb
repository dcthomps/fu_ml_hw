{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7.2 - XGBoost\n",
    "\n",
    "Welcome to the assignment for week 7.\n",
    "\n",
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please state both names of your group members here:\n",
    "Jane and John Doe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paola Gega, Daniel Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.2.1: XGBoost - Regression\n",
    "\n",
    "* Build an XGBoost classifier using `numpy` only. Train your XGBoost model on the `California Housing` regression task. Report on the performance predicting unseen test samples. **(RESULTS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Class structure that might help. Feel free to modify as needed.\n",
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"Decision tree for XGBoost\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_depth,\n",
    "            eta,\n",
    "            min_split_loss,\n",
    "            reg_lambda,\n",
    "            min_samples_split=1,\n",
    "            ):\n",
    "        self.max_depth = max_depth\n",
    "        self.eta = eta\n",
    "        self.min_split_loss = min_split_loss\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _find_best_split(self, X, gradients, hessians):\n",
    "        \"\"\"Find the best feature and threshold to split on\"\"\"\n",
    "        if len(y) <= self.min_samples_split:\n",
    "            return None, None, None\n",
    "\n",
    "        best_feature_index, best_threshold, best_gain = None, None, 0\n",
    "        n_features = X.shape[1]\n",
    "        # Step 1: Sum all gradients and hessians for parent node\n",
    "        leaf_weight = - np.sum(gradients) / np.sum(hessians) + self.reg_lambda\n",
    "\n",
    "        # Step 2: Loop over all features\n",
    "        for feature_index in range(n_features):\n",
    "            # Sort gradients and hessians by feature values\n",
    "            sorted_indices = np.argsort(X[:, feature_index])\n",
    "            X_sorted = X[sorted_indices, feature_index]\n",
    "            gradients_sorted = gradients[sorted_indices]\n",
    "            hessians_sorted = hessians[sorted_indices]\n",
    "\n",
    "            # Step 3: Loop over all possible thresholds\n",
    "            thresholds = self._get_split_candidates(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                # Sum gradients and hessians for potential left and right split\n",
    "                left_indices = X_sorted <= threshold\n",
    "                right_indices = ~left_indices\n",
    "\n",
    "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
    "                    continue\n",
    "                # (Check for minimum child weight constraint, if applicable)\n",
    "                G_left = np.sum(gradients_sorted[left_indices])\n",
    "                H_left = np.sum(hessians_sorted[left_indices])\n",
    "                G_right = np.sum(gradients_sorted[right_indices])\n",
    "                H_right = np.sum(hessians_sorted[right_indices])\n",
    "                \n",
    "                # Step 4: compute gain\n",
    "                left_weight = - G_left / (H_left + self.reg_lambda)\n",
    "                right_weight = - G_right / (H_right + self.reg_lambda)\n",
    "                gain = 0.5 * (G_left**2 / (H_left + self.reg_lambda) \n",
    "                              + G_right**2 / (H_right + self.reg_lambda) \n",
    "                              - (G_left + G_right)**2 / (H_left + H_right + self.reg_lambda)) - self.min_split_loss\n",
    "\n",
    "                # Step 5: store best split based on gain\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature_index, best_threshold, best_gain\n",
    "\n",
    "    def _get_split_candidates(self, feature_values):\n",
    "        unique_feature_values = np.unique(feature_values)\n",
    "        return (unique_feature_values[:-1]+unique_feature_values[1:])/2\n",
    "\n",
    "    def _build_tree(self, X, gradients, hessians, depth=0):\n",
    "        \"\"\"Recursively build the decision tree.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if depth >= self.max_depth:\n",
    "            leaf_weight = - np.sum(gradients) / np.sum(hessians) + self.reg_lambda\n",
    "            return Node(value=leaf_weight)\n",
    "\n",
    "        feature_index, threshold, gain = self._find_best_split(X, gradients, hessians)\n",
    "\n",
    "        if feature_index is None:\n",
    "            leaf_weight = - np.sum(gradients) / np.sum(hessians) + self.reg_lambda\n",
    "            return Node(value=leaf_weight)\n",
    "\n",
    "        left_indices = X[:, feature_index] <= threshold\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices],\n",
    "                                        gradients[left_indices],\n",
    "                                        hessians[left_indices],\n",
    "                                        depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices],\n",
    "                                         gradients[right_indices],\n",
    "                                         hessians[right_indices],\n",
    "                                         depth + 1)\n",
    "\n",
    "        return Node(feature_index=feature_index, threshold=threshold, left=left_subtree, right=right_subtree)\n",
    "    \n",
    "    def fit(self, X, gradients, hessians):\n",
    "        \"\"\"Build the tree\"\"\"\n",
    "        self.root = self._build_tree(X, gradients, hessians)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        \"\"\"Predict the class for a single sample.\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        else:\n",
    "            return self._predict_one(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the tree\"\"\"\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost:\n",
    "    \"\"\"XGBoost implementation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        eta=0.1,\n",
    "        min_split_loss=0,\n",
    "        reg_lambda=1e-5,\n",
    "        criterion=\"mse\",\n",
    "    ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.eta = eta\n",
    "        self.min_split_loss = min_split_loss\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.criterion = criterion  # \"mse\" or \"cross_entropy\"\n",
    "        self.trees = []\n",
    "\n",
    "    def _get_gradients_and_hessians(self, y_true, y_pred):\n",
    "        \"\"\"Compute gradients and Hessians based on the loss function.\"\"\"\n",
    "        if self.criterion == \"mse\":\n",
    "            gradients = y_pred - y_true\n",
    "            hessians = np.ones_like(y_true)\n",
    "        elif self.criterion == \"cross_entropy\":\n",
    "            # For binary classification using logistic loss\n",
    "            y_pred = 1 / (1 + np.exp(-y_pred))  # Sigmoid\n",
    "            gradients = y_pred - y_true\n",
    "            hessians = y_pred * (1 - y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion. Choose 'mse' or 'cross_entropy'.\")\n",
    "        return gradients, hessians\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the XGBoost model\"\"\"\n",
    "        # Initialize the prediction\n",
    "        F_0 = np.mean(y)\n",
    "        F_m = np.full(y.shape, F_0)\n",
    "\n",
    "        for m in range(self.n_estimators):\n",
    "            # Compute gradients and Hessians\n",
    "            gradients, hessians = self._get_gradients_and_hessians(y, F_m)\n",
    "            # Fit a decision tree to the gradients and Hessians\n",
    "            tree = DecisionTree(self.max_depth, self.eta, self.min_split_loss, self.reg_lambda)\n",
    "            tree.fit(X, gradients, hessians)\n",
    "            # Update ensemble\n",
    "            self.trees.append(tree)\n",
    "            F_m += self.eta * tree.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        F_m = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            F_m += self.eta * tree.predict(X)\n",
    "        return F_m\n",
    "    \n",
    "    # Probabilities for classification :) - The Bonus task\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities for binary classification\"\"\"\n",
    "        F_m = np.zeros(X.shape[0])\n",
    "        for tree in self.trees:\n",
    "            F_m += self.eta * tree.predict(X)\n",
    "        probs = 1 / (1 + np.exp(-F_m))  # Sigmoid\n",
    "        return np.vstack([1 - probs, probs]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set: 4.80496077296282\n",
      "R^2 on training set: -2.6010696397706687\n",
      "MSE on test set: 4.815143760528366\n",
      "R^2 on test set: -2.638964538536548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "# Load California Housing data\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "# Don't know if standardizing is necessary or not...\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a XGBoost model\n",
    "model = XGBoost(n_estimators=10, max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict(X_train)\n",
    "print(\"MSE on training set:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"R^2 on training set:\", r2_score(y_train, y_train_pred))\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(\"MSE on test set:\", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"R^2 on test set:\", r2_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.2.2: XGBoost - Classification (BONUS)\n",
    "\n",
    "* Train an XGBoost model on the `Breast Cancer` binary classification task. Report on the performance predicting unseen test samples. **(RESULTS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Access the features and labels\n",
    "X = data.data  # Shape: (569, 30)\n",
    "y = data.target  # Shape: (569,) - 0 for malignant, 1 for benign\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "# Don't know if standardizing is necessary or not...\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a XGBoost model\n",
    "model = XGBoost(n_estimators=10, max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_pred = np.where(model.predict_proba(X_train) >= 0.5, 1, 0)\n",
    "print(\"Accuracy on training set:\", accuracy_score(y_train, y_train_pred))\n",
    "y_test_pred = np.where(model.predict_proba(X_test) >= 0.5, 1, 0)\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
