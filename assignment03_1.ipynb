{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3.1 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit your solution of this notebook in the Whiteboard at the corresponding Assignment entry as .ipynb-file and as .pdf. <br><br>\n",
    "Please do **NOT** rename the file!\n",
    "\n",
    "#### State both names of your group members here:\n",
    "[Jane and John Doe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Thompson, Paola Gega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Grading Info/Details - Assignment 3.1:\n",
    "\n",
    "The assignment will be graded semi-automatically, which means that your code will be tested against a set of predefined test cases and qualitatively assessed by a human. This will speed up the grading process for us.\n",
    "\n",
    "* For passing the test scripts: \n",
    "    - Please make sure to **NOT** alter predefined class or function names, as this would lead to failing of the test scripts.\n",
    "    - Please do **NOT** rename the files before uploading to the Whiteboard!\n",
    "\n",
    "* **(RESULT)** tags indicate checkpoints that will be specifically assessed by a human.\n",
    "\n",
    "* You will pass the assignment if you pass the majority of test cases and we can at least confirm effort regarding the **(RESULT)**-tagged checkpoints per task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1.1 - Linear Regression\n",
    "\n",
    "Linear regression is a supervised learning algorithm that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data.\n",
    "\n",
    "### 1. Linear Regression Implementation\n",
    "* Implement a linear regression model optimized by gradient descent or the closed-form approach presented in the lecture using `numpy` only. Use the `LinearRegression` class structure below. Especially, when using the closed-form, beware of numeric stability issues.**(RESULT)**\n",
    "* Evaluate the performance of your implementation using appropriate metrics for the [AUTO MPG](https://archive.ics.uci.edu/dataset/9/auto+mpg) dataset. Make sure to preprocess the data properly for the regression task. Report on the loss during training the regressor. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, convergence_tol=1e-6, silent=False, bias=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "        self.silent = silent\n",
    "        self.bias = bias\n",
    "        self.convergence_tol = convergence_tol\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''Train the linear regression model (i.e., calculate the vector of weights)\n",
    "        using gradient descent.\n",
    "\n",
    "        We want to minimize the mean squared error loss function:\n",
    "        L(w) = (1/n) * ||y - Z w||^2\n",
    "\n",
    "        where Z is the design matrix (with a column of ones for the intercept term).\n",
    "\n",
    "        If X has correlated features there is not a unique solution for w.\n",
    "        However our understanding is that gradient descent\n",
    "        will still converge to a point within the subspace of solutions.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : numpy array of shape (n_samples,)\n",
    "            The target values.\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "        # First we normalize the features\n",
    "        self.means = np.mean(X, axis=0)\n",
    "        self.stds = np.std(X, axis=0)\n",
    "        self.stds[self.stds==0] = 1.0  # Prevent division by zero for constant features\n",
    "        X = (X - self.means) / self.stds\n",
    "\n",
    "        # Add a column of ones to X for the intercept term\n",
    "        if self.bias:\n",
    "            X = np.column_stack((np.ones((n_samples, 1)), X))\n",
    "        \n",
    "        # Initialize weights\n",
    "        if self.bias:\n",
    "            self.weights = np.zeros(n_features + 1)\n",
    "        else:\n",
    "            self.weights = np.zeros(n_features)\n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iterations):\n",
    "            y_pred = X @ self.weights\n",
    "            errors = y_pred - y\n",
    "            gradient = X.T @ errors / n_samples\n",
    "            self.weights -= self.learning_rate * gradient\n",
    "            # Compute and store loss\n",
    "            loss = np.sum(errors**2) / (2 * n_samples)\n",
    "            self.loss_history.append(loss)\n",
    "            # Decrease learning rate over time\n",
    "            # self.learning_rate *= 0.99\n",
    "            # Check for convergence?\n",
    "            if loss < self.convergence_tol:\n",
    "                if not self.silent:\n",
    "                    print(f\"Converged after {_} iterations.\")\n",
    "                break\n",
    "        # If we exit the loop without converging\n",
    "        else:\n",
    "            if not self.silent:\n",
    "                print(\"Warning: Gradient descent did not converge within the\" \\\n",
    "                \" specified number of iterations.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Make predictions using the trained linear regression model.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns:\n",
    "        y_pred : numpy array of shape (n_samples,)\n",
    "            The predicted values.\n",
    "        '''\n",
    "\n",
    "        # Normalize the features using the stored means and stds\n",
    "        X = (X - self.means) / self.stds\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        # Add a column of ones to X for the intercept term\n",
    "        if self.bias:\n",
    "            X = np.column_stack((np.ones((n_samples, 1)), X))\n",
    "        y_pred = X @ self.weights\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AUTO MPG dataset\n",
    "\n",
    "# Potentially useful imports\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/9/auto+mpg.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 10.7109\n",
      "Coefficient of Determination R^2 on Test Set: 0.7902\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "df = pd.read_csv(z.open(\"auto-mpg.data\"), sep='\\s+', header=None,\n",
    "                 names=[\"mpg\",\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\n",
    "                        \"acceleration\",\"model year\",\"origin\",\"car name\"])\n",
    "# Preprocess data: handle missing values\n",
    "df = df.replace('?', np.nan)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "X = df[[\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\n",
    "        \"acceleration\",\"model year\",\"origin\"]].to_numpy().astype(float)\n",
    "y = df[\"mpg\"].to_numpy().astype(float)\n",
    "\n",
    "# Split into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=100000, silent=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot loss history during training\n",
    "# plt.plot(model.loss_history)\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Mean Squared Error Loss\")\n",
    "# plt.title(\"Loss History during Training\")\n",
    "# plt.show()\n",
    "\n",
    "# Evaluate the model by calculating mean squared error of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "errors = y_test - y_pred\n",
    "ss_residual = np.sum(errors**2)\n",
    "mse = ss_residual / len(y_test)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "# Calculate the coefficient of determination R^2\n",
    "ss_total = np.sum((y_test - np.mean(y_test))**2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "print(f\"Coefficient of Determination R^2 on Test Set: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than imputing values for missing data, we just delete these samples because there are not very many of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1.2 - Polynomial Regression\n",
    "\n",
    "Transform features into polynomial terms (x, x², x³) to fit non-linear relationships.\n",
    "\n",
    "* Implement a feature transformation step before feeding to your linear model. **(RESULT)**\n",
    "* Compare the performance of the polynomial regression model (degree 2 and 3) with the linear model on the [AUTO MPG](https://archive.ics.uci.edu/dataset/9/auto+mpg) dataset. **(RESULT)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "class PolynomialFeatures:\n",
    "    \"\"\"\n",
    "    Transform features into non-constant polynomial features.\n",
    "    \n",
    "    For degree=2 and features [x1, x2]:\n",
    "    Output: [x1, x2, x1², x1*x2, x2²]\n",
    "    \"\"\"\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit method (no action needed for polynomial features).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform input data X into polynomial features.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Generate all combinations of feature indices with replacement\n",
    "        feature_indices = []\n",
    "        for degree in range(1, self.degree + 1):\n",
    "            feature_indices.extend(combinations_with_replacement(range(n_features), degree))\n",
    "        \n",
    "        # Create the design matrix\n",
    "        X_poly = np.ones((n_samples, len(feature_indices)))\n",
    "        for i, indices in enumerate(feature_indices):\n",
    "            for index in indices:\n",
    "                X_poly[:, i] *= X[:, index]\n",
    "        return X_poly\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "class PolynomialRegression(LinearRegression):\n",
    "    \"\"\"Polynomial Regression using Linear Regression with polynomial feature transformation.\"\"\"\n",
    "    def __init__(self, degree=2, learning_rate=0.01, n_iterations=1000, convergence_tol=1e-6, silent=False):\n",
    "        self.poly_features = PolynomialFeatures(degree=degree)\n",
    "        super().__init__(\n",
    "            learning_rate=learning_rate,\n",
    "            n_iterations=n_iterations,\n",
    "            convergence_tol=convergence_tol,\n",
    "            silent=silent,\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit polynomial regression model.\"\"\"\n",
    "        X_poly = self.poly_features.fit_transform(X)\n",
    "        super().fit(X_poly, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        X_poly = self.poly_features.transform(X)\n",
    "        return super().predict(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 7.2649\n",
      "Coefficient of Determination R^2 on Test Set: 0.8577\n"
     ]
    }
   ],
   "source": [
    "# Train a quadratic polynomial regression model\n",
    "model = PolynomialRegression(degree=2, learning_rate=0.01, n_iterations=100000, silent=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot loss history during training\n",
    "# plt.plot(model.linear_model.loss_history)\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Mean Squared Error Loss\")\n",
    "# plt.title(\"Loss History during Training\")\n",
    "# plt.show()\n",
    "\n",
    "# Evaluate the model by calculating mean squared error of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "errors = y_test - y_pred\n",
    "ss_residual = np.sum(errors**2)\n",
    "mse = ss_residual / len(y_test)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "# Calculate the coefficient of determination R^2\n",
    "ss_total = np.sum((y_test - np.mean(y_test))**2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "print(f\"Coefficient of Determination R^2 on Test Set: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 6.9163\n",
      "Coefficient of Determination R^2 on Test Set: 0.8645\n"
     ]
    }
   ],
   "source": [
    "# Train a cubic polynomial regression model\n",
    "model = PolynomialRegression(degree=3, learning_rate=0.01, n_iterations=100000, silent=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model by calculating mean squared error of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "errors = y_test - y_pred\n",
    "ss_residual = np.sum(errors**2)\n",
    "mse = ss_residual / len(y_test)\n",
    "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
    "# Calculate the coefficient of determination R^2\n",
    "ss_total = np.sum((y_test - np.mean(y_test))**2)\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "print(f\"Coefficient of Determination R^2 on Test Set: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report:**\n",
    "\n",
    "All in all, we seem to get a better performance on the test data with a polynomial regression than with a basic linear regression.  Note that when calculating MSE and $R^2$ we are not keeping any of the transformed features, so we really are comparing apples to apples.\n",
    "\n",
    "We did encounter a problem however when training the cubic regression: when using the same learning rate and number of iterations as before our gradient descent algorithm would diverge to infinity and fail.  We were able to remedy this by using a smaller learning rate and allowing more iterations.  Other solutions might be to let the learning rate decay as the iteration count increases.  We may even consider re-standardizing our polynomial-transformed data before fitting, though this idea makes us uncomfortable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
